{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the course: Bayesin Machine Learning in Python: A/B Testing\n",
    "# https://deeplearningcourses.com/c/bayesian-machine-learning-in-python-ab-testing\n",
    "# https://www.udemy.com/bayesian-machine-learning-in-python-ab-testing\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "NUM_TRIALS = 10000\n",
    "EPS = 0.1\n",
    "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]\n",
    "\n",
    "\n",
    "class BanditArm:\n",
    "  def __init__(self, p):\n",
    "    # p: the win rate\n",
    "    self.p = p\n",
    "    self.p_estimate = # TODO\n",
    "    self.N = # TODO\n",
    "\n",
    "  def pull(self):\n",
    "    # draw a 1 with probability p\n",
    "    return np.random.random() < self.p\n",
    "\n",
    "  def update(self, x):\n",
    "    self.N = # TODO\n",
    "    self.p_estimate = # TODO\n",
    "\n",
    "\n",
    "def experiment():\n",
    "  bandits = [BanditArm(p) for p in BANDIT_PROBABILITIES]\n",
    "\n",
    "  rewards = np.zeros(NUM_TRIALS)\n",
    "  num_times_explored = 0\n",
    "  num_times_exploited = 0\n",
    "  num_optimal = 0\n",
    "  optimal_j = np.argmax([b.p for b in bandits])\n",
    "  print(\"optimal j:\", optimal_j)\n",
    "\n",
    "  for i in range(NUM_TRIALS):\n",
    "\n",
    "    # use epsilon-greedy to select the next bandit\n",
    "    if np.random.random() < EPS:\n",
    "      num_times_explored += 1\n",
    "      j = # TODO\n",
    "    else:\n",
    "      num_times_exploited += 1\n",
    "      j = # TODO\n",
    "\n",
    "    if j == optimal_j:\n",
    "      num_optimal += 1\n",
    "\n",
    "    # pull the arm for the bandit with the largest sample\n",
    "    x = bandits[j].pull()\n",
    "\n",
    "    # update rewards log\n",
    "    rewards[i] = x\n",
    "\n",
    "    # update the distribution for the bandit whose arm we just pulled\n",
    "    bandits[j].update(x)\n",
    "\n",
    "    \n",
    "\n",
    "  # print mean estimates for each bandit\n",
    "  for b in bandits:\n",
    "    print(\"mean estimate:\", b.p_estimate)\n",
    "\n",
    "  # print total reward\n",
    "  print(\"total reward earned:\", rewards.sum())\n",
    "  print(\"overall win rate:\", rewards.sum() / NUM_TRIALS)\n",
    "  print(\"num_times_explored:\", num_times_explored)\n",
    "  print(\"num_times_exploited:\", num_times_exploited)\n",
    "  print(\"num times selected optimal bandit:\", num_optimal)\n",
    "\n",
    "  # plot the results\n",
    "  cumulative_rewards = np.cumsum(rewards)\n",
    "  win_rates = cumulative_rewards / (np.arange(NUM_TRIALS) + 1)\n",
    "  plt.plot(win_rates)\n",
    "  plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))\n",
    "  plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  experiment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
