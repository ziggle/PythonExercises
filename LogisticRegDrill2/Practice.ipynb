{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.gopenai.com/a-step-by-step-tutorial-to-linear-classification-using-logistic-regression-in-python-techniques-368c5c3426ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is logistic regression and how does it work in Python?\n",
    "\n",
    "Logistic regression is similar to linear regression in that there is a linear model involved, but it is in the argument of an exponential. This allows it to fit to an S-curve. There is a LogisticRegression object in Python that can be used to perform the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How to prepare data for logistic regression in Python?\n",
    "\n",
    "Good article on the topic.\n",
    "https://medium.com/@edwardnathanwibisono/preparing-your-data-before-modeling-with-logistic-regression-classifier-35146449a6bd#:~:text=Preparing%20your%20Data%20before%20Modeling%20with%20Logistic%20Regression,4.%20There%20is%20No%20Multi-Collinearity%20between%20Features%20\n",
    "\n",
    "The data must have the following characteristics:\n",
    "1. The Target is Binary As Logistic Regression is a classification algorithm, it is a mandatory to have the Target / Response variable value in Binary. For example: Yes or No Churn or Not Churn ...\n",
    "2. There are No Outliers Outliers tend to influence the model performance, in a negative way. ...\n",
    "3. The Features are Independent ...\n",
    "4. There is No Multi-Collinearity between Features ...\n",
    "\n",
    "\n",
    "Generally, we create a train-test split. This allows us to use the scikit-learn objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How to build a logistic regression model using `scikit-learn`?\n",
    "\n",
    "It is all about the object.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "logisticRegr.predict(x_test[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to evaluate the logistic regression modelâ€™s performance in Python?\n",
    "\n",
    "https://www.tutorialspoint.com/how-to-evaluate-a-logistic-regression-model#:~:text=A%20logistic%20regression%20model's%20performance%20may%20be%20evaluated%20using%20a,information%20criteria%2C%20and%20sensitivity%20analysis.\n",
    "\n",
    "You can still make residual plots and check the cross-validation results to ensure over-fitting is not occurring. You can then look at confusion matrix and ROC curve results.\n",
    "\n",
    "Calibration curves are mentioned. I need to understand those better.\n",
    "\n",
    "https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How to plot the ROC curve in Python for logistic regression?\n",
    "\n",
    "In Sci-kit learn,\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How to handle imbalanced data in logistic regression?\n",
    "\n",
    "Reference: https://statisticalhorizons.com/logistic-regression-for-rare-events/\n",
    "\n",
    "From SO: Balance in the Training Set\n",
    "\n",
    "For logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in King and Zeng (2001) [PDF].\n",
    "\n",
    "These 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.\n",
    "\n",
    "Making Predictions\n",
    "\n",
    "On a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold.\n",
    "\n",
    "Balance in the Training Set\n",
    "\n",
    "For logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in King and Zeng (2001) [PDF].\n",
    "\n",
    "These 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.\n",
    "\n",
    "Making Predictions\n",
    "\n",
    "On a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold.\n",
    "\n",
    "Here is a discussion of using weighting: https://towardsdatascience.com/weighted-logistic-regression-for-imbalanced-dataset-9a5cd88e68b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
