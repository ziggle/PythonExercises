{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.gopenai.com/a-step-by-step-tutorial-to-linear-classification-using-logistic-regression-in-python-techniques-368c5c3426ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is logistic regression and how does it work in Python?\n",
    "\n",
    "Logistic regression is similar to linear regression in that there is a linear model involved, but it is in the argument of an exponential. This allows it to fit to an S-curve. There is a LogisticRegression object in Python that can be used to perform the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How to prepare data for logistic regression in Python?\n",
    "\n",
    "Good article on the topic.\n",
    "https://medium.com/@edwardnathanwibisono/preparing-your-data-before-modeling-with-logistic-regression-classifier-35146449a6bd#:~:text=Preparing%20your%20Data%20before%20Modeling%20with%20Logistic%20Regression,4.%20There%20is%20No%20Multi-Collinearity%20between%20Features%20\n",
    "\n",
    "The data must have the following characteristics:\n",
    "1. The Target is Binary As Logistic Regression is a classification algorithm, it is a mandatory to have the Target / Response variable value in Binary. For example: Yes or No Churn or Not Churn ...\n",
    "2. There are No Outliers Outliers tend to influence the model performance, in a negative way. ...\n",
    "3. The Features are Independent ...\n",
    "4. There is No Multi-Collinearity between Features ...\n",
    "\n",
    "\n",
    "Generally, we create a train-test split. This allows us to use the scikit-learn objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How to build a logistic regression model using `scikit-learn`?\n",
    "\n",
    "It is all about the object.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "logisticRegr.predict(x_test[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to evaluate the logistic regression model’s performance in Python?\n",
    "\n",
    "https://www.tutorialspoint.com/how-to-evaluate-a-logistic-regression-model#:~:text=A%20logistic%20regression%20model's%20performance%20may%20be%20evaluated%20using%20a,information%20criteria%2C%20and%20sensitivity%20analysis.\n",
    "\n",
    "You can still make residual plots and check the cross-validation results to ensure over-fitting is not occurring. You can then look at confusion matrix and ROC curve results.\n",
    "\n",
    "Calibration curves are mentioned. I need to understand those better.\n",
    "\n",
    "https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How to plot the ROC curve in Python for logistic regression?\n",
    "\n",
    "In Sci-kit learn,\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How to handle imbalanced data in logistic regression?\n",
    "\n",
    "Reference: https://statisticalhorizons.com/logistic-regression-for-rare-events/\n",
    "\n",
    "From SO: Balance in the Training Set\n",
    "\n",
    "For logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in King and Zeng (2001) [PDF].\n",
    "\n",
    "These 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.\n",
    "\n",
    "Making Predictions\n",
    "\n",
    "On a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold.\n",
    "\n",
    "Balance in the Training Set\n",
    "\n",
    "For logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in King and Zeng (2001) [PDF].\n",
    "\n",
    "These 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.\n",
    "\n",
    "Making Predictions\n",
    "\n",
    "On a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold.\n",
    "\n",
    "Here is a discussion of using weighting: https://towardsdatascience.com/weighted-logistic-regression-for-imbalanced-dataset-9a5cd88e68b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How to perform hyperparameter tuning in logistic regression models?\n",
    "\n",
    "See this page for a reference: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\n",
    "It says that Logistic Regression does not have really critical hyperparameters. Three were listed:\n",
    "\n",
    "* solver choice\n",
    "* penalty\n",
    "* penalty strength\n",
    "\n",
    "The standard grid search, random search are available. There also is Optuna, which fully automates the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How to implement logistic regression with categorical features in Python?\n",
    "\n",
    "Sure, you just need to encode them as binary -- basically one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How to visualize logistic regression results using `matplotlib`?\n",
    "\n",
    "stackoverflow discusses it: https://stackoverflow.com/questions/69446699/matplotlib-plot-curve-logistic-regression\n",
    "import numpy as np\n",
    "X_train_sorted = np.sort(X_train)\n",
    "y_train_sorted = model.predict_proba(X_train_sorted)\n",
    "plt.scatter(X_train_sorted, y_train_sorted)\n",
    "plt.plot(X_train_sorted, y_train_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How to interpret the coefficients of a logistic regression model?\n",
    "\n",
    "The logistic regression coefficient β associated with a predictor X is the expected change in log odds of having the outcome per unit change in X. So increasing the predictor by 1 unit (or going from 1 level to the next) multiplies the odds of having the outcome by eβ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What are the limitations and challenges of logistic regression?\n",
    "\n",
    "Some limitations of logistic regression are12:\n",
    "It requires the correct identification of independent variables that have a significant impact on the outcome variable. If the wrong variables are included or excluded, the model will have low predictive power.\n",
    "It can only handle binary or categorical outcome variables, not continuous ones. It also assumes a linear relationship between the outcome and the independent variables, which may not always hold true in reality.\n",
    "It requires independent observations, meaning that the outcome of one observation should not affect the outcome of another. This may not be the case for some types of data, such as longitudinal or clustered data.\n",
    "It is prone to overfitting the model, meaning that it may capture noise or random fluctuations in the data rather than the true underlying patterns. This can lead to poor generalization and inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. How to use logistic regression for multiclass classification in Python?\n",
    "\n",
    "Basically, you can turn you multiple classes into columns (like onene-hot encoding). Then it multiple binary classification problems.\n",
    "\n",
    "https://pub.towardsai.net/logistic-regression-for-multi-class-classification-hands-on-with-scikit-learn-bcc0bbad1def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
